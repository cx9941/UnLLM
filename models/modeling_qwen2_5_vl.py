#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_qwen2_5_vl.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from dataclasses import dataclass
from typing import Any, Callable, Optional, Union
from typing import List, Optional, Tuple, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss

from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, DynamicCache
from transformers.generation import GenerationMixin
from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_layers import GradientCheckpointingLayer
from transformers.modeling_outputs import BaseModelOutputWithPast, ModelOutput
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.processing_utils import Unpack
from transformers.utils import LossKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging
from transformers.models.qwen2_5_vl.configuration_qwen2_5_vl import Qwen2_5_VLConfig, Qwen2_5_VLTextConfig, Qwen2_5_VLVisionConfig

from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import KwargsForCausalLM, Qwen2_5_VLCausalLMOutputWithPast, Qwen2_5_VLPreTrainedModel, Qwen2_5_VLModel
from transformers.modeling_outputs import (
    CausalLMOutputWithPast,
)
from .utils import sample_tail_normal_distribution, orthogonality_loss, contrastive_loss

logger = logging.get_logger(__name__)
from transformers.utils import ModelOutput
@dataclass
class CausalLMOutputWithPast(ModelOutput):
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    scores: torch.FloatTensor = None
    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None
    token_type_ids: Optional[Tuple[torch.FloatTensor, ...]] = None
    lm_head_weight: Optional[Tuple[torch.FloatTensor, ...]] = None

from modelscope import AutoProcessor
tokenizer = AutoProcessor.from_pretrained("llms/Qwen2.5-VL-7B-Instruct").tokenizer
class Qwen2_5_VLForConditionalGeneration(Qwen2_5_VLPreTrainedModel, GenerationMixin):
    _checkpoint_conversion_mapping = {
        "^visual": "model.visual",
        r"^model(?!\.(language_model|visual))": "model.language_model",
    }
    _tied_weights_keys = ["lm_head.weight"]

    def __init__(self, config):
        super().__init__(config)
        self.model = Qwen2_5_VLModel(config)
        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)
        self.queue_size = 20
        self.register_buffer("prototypes", torch.randn(200, self.queue_size, 3, config.hidden_size))
        self.register_buffer("queue_ptr",torch.zeros(200, dtype=torch.long))
        self.register_buffer("queue_counts",torch.zeros(200, dtype=torch.long))
        self.post_init()

    def get_input_embeddings(self):
        return self.model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.model.set_input_embeddings(value)

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model.set_decoder(decoder)

    def get_decoder(self):
        return self.model.get_decoder()

    def get_video_features(
        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None
    ):
        return self.model.get_video_features(pixel_values_videos, video_grid_thw)

    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):
        return self.model.get_image_features(pixel_values, image_grid_thw)

    # Make modules available throught conditional class for BC
    @property
    def language_model(self):
        return self.model.language_model

    @property
    def visual(self):
        return self.model.visual

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[list[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        pixel_values: Optional[torch.Tensor] = None,
        pixel_values_videos: Optional[torch.FloatTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        rope_deltas: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        second_per_grid_ts: Optional[torch.Tensor] = None,
        **kwargs: Unpack[KwargsForCausalLM],
    ) -> Union[tuple, Qwen2_5_VLCausalLMOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):
            The tensors corresponding to the input videos. Pixel values can be obtained using
            [`AutoImageProcessor`]. See [`Qwen2VLImageProcessor.__call__`] for details. [`Qwen2_5_VLProcessor`] uses
            [`Qwen2VLImageProcessor`] for processing videos.
        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
            The temporal, height and width of feature shape of each image in LLM.
        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
            The temporal, height and width of feature shape of each video in LLM.
        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):
            The rope index difference between sequence length and multimodal rope.
        second_per_grid_ts (`torch.Tensor` of shape `(num_videos)`, *optional*):
            The time interval (in seconds) for each grid along the temporal dimension in the 3D position IDs.

        Example:

        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration

        >>> model = Qwen2_5_VLForConditionalGeneration.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")
        >>> processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")

        >>> messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": "What is shown in this image?"},
                ],
            },
        ]
        >>> url = "https://www.ilankelman.org/stopsigns/australia.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)

        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ..."
        ```"""

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )

        outputs = self.model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            pixel_values_videos=pixel_values_videos,
            image_grid_thw=image_grid_thw,
            video_grid_thw=video_grid_thw,
            second_per_grid_ts=second_per_grid_ts,
            position_ids=position_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=True,
            cache_position=cache_position,
            **kwargs,
        )

        hidden_states = outputs[0]
        logits = self.lm_head(hidden_states.to(dtype=self.lm_head.weight.dtype))

        loss = None
        # if labels is not None:
        #     loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size)

        # return Qwen2_5_VLCausalLMOutputWithPast(
        #     loss=loss,
        #     logits=logits,
        #     past_key_values=outputs.past_key_values,
        #     hidden_states=outputs.hidden_states,
        #     attentions=outputs.attentions,
        #     rope_deltas=outputs.rope_deltas,
        # )

        if 'token_type_ids' not in kwargs:
            return CausalLMOutputWithPast(
                logits=logits,
                past_key_values=outputs.past_key_values
            )
        
        loss_fct = CrossEntropyLoss()


        shift_logits = logits[..., :-1, :].contiguous()
        shift_logits = shift_logits.view(-1, self.config.text_config.vocab_size)

        gen_labels = input_ids.clone()
        gen_labels[kwargs['token_type_ids']<=0] = -100
        shift_gen_labels = gen_labels[..., 1:].contiguous().view(-1).to(shift_logits.device)
        loss_gen = loss_fct(shift_logits, shift_gen_labels)

        class_labels = input_ids.clone()
        class_labels[kwargs['token_type_ids']!=3] = -100
        shift_class_labels = class_labels[..., 1:].contiguous().view(-1).to(shift_logits.device)
        loss_class = loss_fct(shift_logits, shift_class_labels)

        labels[kwargs['token_type_ids']<=1] = -100

        loss = loss_gen + loss_class

        shift_hidden_states = hidden_states[:,:-1,:].contiguous().to(shift_logits.device)
        shift_token_type_ids = kwargs['token_type_ids'][...,1:].contiguous().to(shift_logits.device)
        class_hidden_states = shift_hidden_states[shift_token_type_ids==3].reshape(logits.shape[0], -1, shift_hidden_states.shape[-1])

        # if self.training:
        
        #     ood_hidden_states = class_hidden_states[:,-1]
            
        #     if kwargs['con_weight'].mean() != 0:
        #         loss_con = contrastive_loss(ood_hidden_states, kwargs['class_golds'], temperature=kwargs['temperature'].mean().item())
        #         loss += loss_con * kwargs['con_weight'].mean()

            # epsilon = int(kwargs['epsilon'].mean())

            # self.propotype_update(class_hidden_states, kwargs['class_golds'], num_samples=kwargs['vos_neg_sample'].mean().int().item(), epsilon=epsilon)

            # self.propotype_update(class_hidden_states, kwargs['class_golds'])


            # if kwargs['neg_weight'].mean() != 0:

            #     neg_sample = self.sample_tail_from_prototypes(num_samples=kwargs['vos_neg_sample'].mean().int().item(), epsilon=epsilon, max_tries=1)
            #     if neg_sample.shape[0] > 0:
            #         pos_sample = class_hidden_states[kwargs['class_golds'] != 0]
            #         loss_neg = orthogonality_loss(neg_sample=neg_sample, pos_sample=pos_sample, k=int(kwargs['e_dim'].mean()))
            #         loss += loss_neg * kwargs['neg_weight'].mean()

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=class_hidden_states,
            token_type_ids=kwargs['token_type_ids'],
            lm_head_weight=self.lm_head.weight[kwargs['class_token'][0]].unsqueeze(0)
        )


    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        attention_mask=None,
        inputs_embeds=None,
        cache_position=None,
        position_ids=None,
        use_cache=True,
        pixel_values=None,
        pixel_values_videos=None,
        image_grid_thw=None,
        video_grid_thw=None,
        second_per_grid_ts=None,
        **kwargs,
    ):
        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model

        model_inputs = super().prepare_inputs_for_generation(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            inputs_embeds=inputs_embeds,
            cache_position=cache_position,
            position_ids=position_ids,
            pixel_values=pixel_values,
            pixel_values_videos=pixel_values_videos,
            image_grid_thw=image_grid_thw,
            video_grid_thw=video_grid_thw,
            second_per_grid_ts=second_per_grid_ts,
            use_cache=use_cache,
            **kwargs,
        )

        # Qwen2-5-VL position_ids are prepareed with rope_deltas in forward
        model_inputs["position_ids"] = None

        if cache_position[0] != 0:
            model_inputs["pixel_values"] = None
            model_inputs["pixel_values_videos"] = None

        return model_inputs

    def _get_image_nums_and_video_nums(
        self,
        input_ids: Optional[torch.LongTensor],
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Get the number of images and videos for each sample to calculate the separation length of the sample tensor.
        These parameters are not passed through the processor to avoid unpredictable impacts from interface modifications.

        Args:
            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
                Indices of input sequence tokens in the vocabulary.

        Returns:
            image_nums (`torch.LongTensor` of shape `(batch_size, num_images_sample)`)
            video_nums (`torch.LongTensor` of shape `(batch_size, num_videos_sample)`)
        """
        image_token_id = self.config.image_token_id
        video_token_id = self.config.video_token_id
        vision_start_token_id = self.config.vision_start_token_id

        vision_start_mask = input_ids == vision_start_token_id
        vision_first_mask = torch.roll(vision_start_mask, shifts=1, dims=1)
        image_mask = input_ids == image_token_id
        video_mask = input_ids == video_token_id
        image_nums = torch.sum(vision_first_mask & image_mask, dim=1)
        video_nums = torch.sum(vision_first_mask & video_mask, dim=1)

        return image_nums, video_nums

    def _expand_inputs_for_generation(
        self,
        expand_size: int = 1,
        is_encoder_decoder: bool = False,
        input_ids: Optional[torch.LongTensor] = None,
        **model_kwargs,
    ) -> tuple[torch.LongTensor, dict[str, Any]]:
        # Overwritten -- Support for expanding tensors without a batch size dimension
        # e.g., pixel_values, image_grid_thw, pixel_values_videos, video_grid_thw, second_per_grid_t
        # pixel_values.shape[0] is sum(seqlen_images for samples)
        # image_grid_thw.shape[0] is sum(num_images for samples)

        if expand_size == 1:
            return input_ids, model_kwargs

        visual_keys = ["pixel_values", "image_grid_thw", "pixel_values_videos", "video_grid_thw", "second_per_grid_ts"]

        def _expand_dict_for_generation_visual(dict_to_expand):
            image_grid_thw = model_kwargs.get("image_grid_thw", None)
            video_grid_thw = model_kwargs.get("video_grid_thw", None)
            image_nums, video_nums = self._get_image_nums_and_video_nums(input_ids)

            def _repeat_interleave_samples(x, lengths, repeat_times):
                samples = torch.split(x, lengths)
                repeat_args = [repeat_times] + [1] * (x.dim() - 1)
                result = torch.cat([sample.repeat(*repeat_args) for sample in samples], dim=0)
                return result

            for key in dict_to_expand:
                if key == "pixel_values":
                    # split images into samples
                    samples = torch.split(image_grid_thw, list(image_nums))
                    # compute the sequence length of images for each sample
                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "image_grid_thw":
                    # get the num of images for each sample
                    lengths = list(image_nums)
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "pixel_values_videos":
                    samples = torch.split(video_grid_thw, list(video_nums))
                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "video_grid_thw":
                    lengths = list(video_nums)
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "second_per_grid_ts":
                    if not isinstance(dict_to_expand[key], list):
                        raise TypeError(
                            f"Expected value for key '{key}' to be a list, but got {type(dict_to_expand[key])} instead."
                        )
                    tensor = torch.tensor(dict_to_expand[key])
                    lengths = list(video_nums)
                    tensor = _repeat_interleave_samples(tensor, lengths=lengths, repeat_times=expand_size)
                    dict_to_expand[key] = tensor.tolist()
            return dict_to_expand

        def _expand_dict_for_generation(dict_to_expand):
            for key in dict_to_expand:
                if (
                    key != "cache_position"
                    and dict_to_expand[key] is not None
                    and isinstance(dict_to_expand[key], torch.Tensor)
                    and key not in visual_keys
                ):
                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)
            return dict_to_expand

        # input_ids is required for expanding visual inputs
        # If input_ids is unavailable, visual inputs will not be used; therefore, there is no need to expand visual inputs.
        if input_ids is not None and input_ids.numel() != 0:
            model_kwargs = _expand_dict_for_generation_visual(model_kwargs)

        if input_ids is not None:
            input_ids = input_ids.repeat_interleave(expand_size, dim=0)

        model_kwargs = _expand_dict_for_generation(model_kwargs)

        if is_encoder_decoder:
            if model_kwargs.get("encoder_outputs") is None:
                raise ValueError("If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.")
            model_kwargs["encoder_outputs"] = _expand_dict_for_generation(model_kwargs["encoder_outputs"])

        return input_ids, model_kwargs



    def propotype_update(self, class_hidden_states, class_golds):
        """
        æ›´æ–°æ¯ä¸€ç±»çš„ prototype queueï¼Œä½¿ç”¨å¾ªçŽ¯æŒ‡é’ˆæ’å…¥ï¼Œé¿å…é¢‘ç¹ append/popã€‚
        """
        with torch.no_grad():
            for class_idx in class_golds[(class_golds !=0)].unique():
                cls_mask = (class_golds == class_idx)                   # [B]
                cls_idx = class_idx.item()
                cls_features = class_hidden_states[cls_mask]      # [Nc, dim]
                n = cls_features.size(0)

                # å¦‚æžœè¶…è¿‡ queue_sizeï¼Œä»…ä¿ç•™æœ€æ–° queue_size ä¸ªæ ·æœ¬
                if n > self.queue_size:
                    cls_features = cls_features[-self.queue_size:]
                    n = self.queue_size

                ptr = self.queue_ptr[cls_idx].item()
                end_ptr = (ptr + n) % self.queue_size

                if ptr + n <= self.queue_size:
                    self.prototypes[cls_idx, ptr:ptr + n] = cls_features.detach()
                else:
                    first_part = self.queue_size - ptr
                    self.prototypes[cls_idx, ptr:] = cls_features[:first_part].detach()
                    self.prototypes[cls_idx, :end_ptr] = cls_features[first_part:].detach()

                # update pointer and count
                self.queue_ptr[cls_idx] = end_ptr
                self.queue_counts[cls_idx] = min(self.queue_counts[cls_idx] + n, self.queue_size)




    def sample_tail_from_prototypes(self, num_samples=50, epsilon=3, max_tries=1):
        """
        ä»Žæ¯ç±» prototypeæž„å»ºé«˜æ–¯åˆ†å¸ƒï¼Œåœ¨æ¯ç±»ä¸Šé‡‡æ ·è¿œç¦»å‡å€¼çš„å°¾éƒ¨æ ·æœ¬ã€‚
        """
        with torch.no_grad():

            mask = self.queue_counts==self.queue_size
            queue_prototypes = self.prototypes[mask].detach()
            C, N, D1, D2 = queue_prototypes.shape
            results = torch.tensor([], dtype=queue_prototypes.dtype, device=queue_prototypes.device)
            
            if C == 0:
                return results

            # Step 1: è®¡ç®—å‡å€¼å’Œæ–¹å·®
            mean = queue_prototypes.mean(dim=1)    # [N, D]
            std = queue_prototypes.std(dim=1) + 1e-6  # [N, D]ï¼Œé˜²æ­¢é™¤é›¶
            # ä½¿ç”¨ torch.cdist è®¡ç®— L2 è·ç¦»ï¼ˆéžå¹³æ–¹ï¼‰
            queue_flat = queue_prototypes.view(queue_prototypes.size(0), queue_prototypes.size(1), -1)  # (C, N, D)
            mean_flat = mean.view(mean.size(0), -1).unsqueeze(1)                                        # (C, 1, D)
            std_flat = std.view(mean.size(0), -1).unsqueeze(1)                                        # (C, 1, D)
            origin_dists = (((queue_flat - mean_flat) / std_flat) ** 2).sum(dim=-1).sqrt()  # [C, N]
            max_dist = origin_dists.topk(k=epsilon, dim=1).values[:, -1].unsqueeze(-1)                  # (C, 1)
            del origin_dists
            torch.cuda.empty_cache()
            for _ in range(max_tries):
                # Step 2: é‡‡æ · [N, num_samples * 100, D]
                z = torch.randn(C, num_samples * 100, D1, D2, device=queue_prototypes.device)
                # # Step 3: è·ç¦»è®¡ç®— [N, B]
                samples = mean.unsqueeze(1) + std.unsqueeze(1) * z
                scores = z.view(z.shape[0], z.shape[1], -1).norm(p=2, dim=-1)
                neg_samples = samples[scores >= max_dist]                 # tail region
                results = torch.concat([results, neg_samples], dim=0)
                if results.shape[0] > num_samples:
                    break

        return results